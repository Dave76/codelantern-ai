---
title: "v0 Model API"
source: "https://v0.app/docs/api/model"
---

The v0 Model API is designed for building modern web applications. It supports text and image inputs, provides fast streaming responses, and is compatible with the [OpenAI Chat Completions API](https://platform.openai.com/docs/api-reference/chat) format.

## Features

* **Framework aware completions**: Evaluated on modern stacks like Next.js and Vercel.
* **Auto-fix**: Identifies and corrects common coding issues during generation.
* **Quick edit**: Streams inline edits as they’re available.
* **OpenAI compatible**: Can be used with any tool or SDK that supports OpenAI's API format.
* **Multimodal**: Supports both text and image inputs (base64-encoded image data).

You can experiment with the Models API in the AI Playground to test prompts and view responses:

* [v0-1.5-md](https://ai-sdk.dev/playground/vercel:v0-1.5-md)
* [v0-1.5-lg](https://ai-sdk.dev/playground/vercel:v0-1.5-lg)
* [v0-1.0-md](https://ai-sdk.dev/playground/vercel:v0-1.0-md)

## Getting started

The v0 API is currently in beta and requires a Premium or Team plan with usage-based billing enabled. For details, visit the [pricing page](/docs/pricing).

To start using the Models API, create an API key on [v0.app](https://v0.app/chat/settings/keys).

You can then integrate it using the [AI SDK](https://ai-sdk.dev/), a TypeScript library designed for working with and other OpenAI-compatible models.

```bash
pnpm add ai @ai-sdk/vercel
```

### Example usage

```ts
import { generateText } from 'ai'
import { vercel } from '@ai-sdk/vercel'

const { text } = await generateText({
  model: vercel('v0-1.0-md'),
  prompt: 'Create a Next.js AI chatbot with authentication',
})
```

## Models

### v0-1.5-md

The `v0-1.5-md` model is for everyday tasks and UI generation.

### v0-1.5-lg

The `v0-1.5-lg` model is for advanced thinking or reasoning.

### v0-1.0-md (legacy)

The `v0-1.0-md` model is the legacy model served by the API.

**Capabilities:**

* Supports text and image inputs (multimodal)
* Compatible with OpenAI’s Chat Completions format
* Supports function/tool calls
* Streaming responses with low latency
* Optimized for frontend and full-stack web development

## API reference

### Endpoint

```
POST https://api.v0.dev/v1/chat/completions
```

This endpoint generates a model response based on a list of messages.

### Headers

| Header        | Required | Description                        |
| ------------- | -------- | ---------------------------------- |
| Authorization | Yes      | Bearer token: `Bearer $V0_API_KEY` |
| Content-Type  | Yes      | Must be `application/json`         |

### Request body

| Field                   | Type             | Required | Description                                                        |
| ----------------------- | ---------------- | -------- | ------------------------------------------------------------------ |
| `model`                 | string           | Yes      | Model name.                                                        |
| `messages`              | array            | Yes      | List of message objects forming the conversation.                  |
| `stream`                | boolean          | No       | If true, the response will be returned as a stream of data chunks. |
| `tools`                 | array            | No       | Optional tool definitions (e.g., functions or API calls).          |
| `tool_choice`           | string or object | No       | Specifies which tool to call, if tools are provided.               |
| `max_completion_tokens` | integer          | No       | Maximum generated output tokens (default: `4000`).                 |

Each message object must contain:

| Field     | Type            | Required | Description                                                         |
| --------- | --------------- | -------- | ------------------------------------------------------------------- |
| `role`    | string          | Yes      | One of `"user"`, `"assistant"`, or `"system"`.                      |
| `content` | string or array | Yes      | The message content. Can be a string or array of text/image blocks. |

### Example request

```bash
curl https://api.v0.dev/v1/chat/completions \
  -H "Authorization: Bearer $V0_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "v0-1.5-md",
    "messages": [
      { "role": "user", "content": "Create a Next.js AI chatbot" }
    ]
  }'
```

### Example with streaming

```bash
curl https://api.v0.dev/v1/chat/completions \
  -H "Authorization: Bearer $V0_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "v0-1.5-md",
    "stream": true,
    "messages": [
      { "role": "user", "content": "Add login to my Next.js app" }
    ]
  }'
```

### Response

If `stream` is `false` (default), the response is a JSON object:

```json
{
  "id": "v0-123",
  "model": "v0-1.5-md",
  "object": "chat.completion",
  "created": 1715620000,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Here's how to add login to your Next.js app..."
      },
      "finish_reason": "stop"
    }
  ]
}
```

If `stream` is `true`, the server returns a series of data chunks formatted as [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events). Each line begins with `data: ` followed by a partial delta:

```json
{
  "id": "v0-123",
  "model": "v0-1.5-md",
  "object": "chat.completion.chunk",
  "choices": [
    {
      "delta": {
        "role": "assistant",
        "content": "Here's how"
      },
      "index": 0,
      "finish_reason": null
    }
  ]
}
```

## Usage limits

| Model     | Max context window size | Max output context size |
| --------- | ----------------------- | ----------------------- |
| v0-1.0-md | 128,000 tokens          | 64,000 tokens           |
| v0-1.5-md | 128,000 tokens          | 64,000 tokens           |
| v0-1.5-lg | 512,000 tokens          | 64,000 tokens           |

To request a higher limit, contact us at [support@v0.dev](mailto:support@v0.dev).

By using our API, you agree to our [API Terms](https://vercel.com/legal/api-terms).
